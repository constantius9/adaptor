\section{Проверка оптимизаций, сохраняющих структуру программы}

Опишем стратегию инструмента проверки \emph{voc} \cite{ZPFG02} и теорию проверки оптимизаций.

Компилятор получает на вход \emph{исходную программу}, написанную на высокоуровневом языке, преобразует её в \emph{промежуточное представление} и затем применяет набор оптимизаций к программе --- начиная с классических глобальных оптимизаций, независящих от архитектуры, и заканчивая архитектурно"~зависимыми, такими как распределение регистров и планирование инструкций. Обычно эти оптимизации производятся в несколько проходов (до 15 в некоторых компиляторах), и каждый проход применяет оптимизацию определённого типа.

Для того, чтобы доказать, что целевая программа является корректной трансляцией исходной программы, рассмотрим сначала некоторые необходимые термины. Определим, что целевая программа \emph{T} является корректной трансляцией исходной программы \emph{S}, если исполнение программы \emph{T} с некоторыми значениями переменных эквивалентно исполнению программы \emph{S} с некоторыми другими значениями переменных.

Промежуточное представление --- это трёхадресный код. Он описан \emph{графом потока}. Каждый узел в графе потока представляет собой \emph{линейный участок}, то есть последовательность операторов, не содержащую ветвлений. Границы линейного участка определяются потоком управления.

\subsection{Системы переходов}

Для доказательства правильности преобразования исходного кода, выделим сначала семантически эквивалентные последовательности операций в коде до преобразования и после. Это осуществляется с помощью \emph{системы переходов} (англ. transition system, $TS$; вариант понятия, аналогичного упомянутому в работе \cite{PSS98b}). Система переходов $S = \left\langle  V, O, \Theta, \rho \right\rangle $ --- конечный автомат, где $V$ --- множество переменных состояния, $O \subseteq V$ --- множество наблюдаемых переменных состояния, $\Theta$ --- начальное состояние системы, $\rho$ --- правила перехода, сопоставляющие исходное и результирующее состояние.

Значение, получаемое переменной $x$ в состоянии $s$, обозначается $s[x]$. Правило перехода $y' = y + 1$ означает, что в новом состоянии значение переменной $y$ на 1 больше, чем в старом.

Наблюдаемыми считаются все переменные, значение которых выводится на внешние устройства в результате работы программы.

Система переходов называется детерминированной, если начальное состояние однозначно определяет дальнейший ход вычислений. Мы рассматриваем проверку только таких программ.

Определим $P_{S} = \left\langle  V_{s}, O_{s}, \Theta_{s}, \rho_{s} \right\rangle $ и $P_{T} = \left\langle  V_{t}, O_{t}, \Theta_{t}, \rho_{t} \right\rangle $ как исходную и целевую систему переходов, соответственно. 
Они называются сравнимыми, если существует соотношение один"~к"~одному между наблюдаемыми переменными $O_{S}$ системы $P_{S}$ и наблюдаемыми переменными $O_{T}$ системы $P_{T}$. Обозначим $ X \subset O_{s} $ и $ x \subset O_{t} $ соответствующие наблюдаемые переменные. 
Исходное состояние \emph{совместимо} с целевым, если они оба включают одинаковые множества наблюдаемых переменных. Обозначим $\sigma_{S}: s_{0}, s_{1}, \ldots, s_{m}$ вычисление в исходной системе переходов $P_{S}$ и $\sigma_{T}: t_{0}, t_{1}, \ldots, t_{n}$ вычисление в целевой системе переходов $P_{T}$. 

{
    Рассмотрим условия, при которых трансляция программы считается правильной.
    \begin{enumerate}
        \item {Пусть $P_{S}$ и $P_{T}$ сравнимы. \label{st:comparable}}
        \item {Положим также, что начальные состояния $s_{0}$ и $t_{0}$ вычислений $\sigma_{S}$ и $\sigma_{T}$ соответственно совместимы. \label{st:compatible}}
        \item {Будем считать, что если вычисление $\sigma_{S}$ завершается, то вычисление $\sigma_{T}$ также должно завершаться и их конечные состояния $s_{m}$ и $t_{n}$ соответственно должны быть совместимы. \label{st:terminating}}
    \end{enumerate}
    
    В случае выполнения условий \ref{st:comparable}, \ref{st:compatible}, \ref{st:terminating} система $P_{T}$ является правильной трансляцией системы $P_{T}$.
}

\subsection{Проверка оптимизаций}
\label{ss:validate}

Положим $P_{S}$ и $P_{T}$ сравнимыми системами переходов. Для формального доказательства корректности преобразования $P_{s}$ в $P_{t}$ введём правило доказательства $Validate$, приведённое ниже. Оно включает в себя установление соответствия потока управления между исходным и целевым кодом и установление соответствия данных между исходным и целевым кодом.

Далее для краткости будем пользоваться термином «участок» как синонимом термина «линейный участок».

Обе системы переходов имеют множество разрезов $CP$. Множество разрезов $CP$ представляет собой множество участков, включающих начальный и конечный участок и хотя бы один участок из каждого цикла в графе потока управления программы. Простой путь --- путь, соединяющий два разреза, и не содержащий больше никаких разрезов в качестве промежуточных узлов.

Далее приводится схема алгоритма $Validate$. За ней следуют дополнительные пояснения.

\begin{enumerate}
    \item Установить соответствие потока управления $CP_{T} \rightarrow CP_{S}$ между начальным и конечным участком в целевом коде и соответствующими участками в исходном коде.
    \item Для каждого участка $B_{i}$ в $CP_{T}$ сформировать инвариант $\phi_{i}$ (см. пояснения после текста схемы алгоритма). \label{st:invariant}
    \item Назначить выражения, содержащие переменные состояния из целевого представления программы, в соответствие переменным. Эти выражения зависят от значений предикатов. Значения предикатов вычисляются из переменных в целевом представлении программы. Таким образом устанавливается соответствие между данными в исходном представлении и целевом представлении программы.
    \item Для каждой пары линейных участков $B_{i}$ и $B_{j}$, для которых существует простой путь в графе потока управления, создать множество простых путей, соединяющих эти линейные участки в графе потока данных, и условие корректности.
    \item Установить верность всех созданных условий корректности трансляции.
\end{enumerate}

Для работы алгоритма программа дополнена знаниями о выполнении условий, наложенных на определённые переменные. Эти знания называются \emph{аннотациями} и имеют вид инвариантов $\phi_{i}$ в пункте \ref{st:invariant} правила. Инвариант --- это условие, которое должно соблюдаться независимо от преобразований, выполняемых над программой.

\label{ss:invariants}
Имеют место разные аннотации для разных линейных участков. Аннотации программы, закреплённые инвариантом $\phi_{i}$, верны при исполнении блока $B_{i}$. Инварианты могут быть получены путём анализа потока данных, который выполняется компилятором. Они представляют собой условия вида $x == 1$, которые должны быть верны в определённых точках исполнения как исходной, так и конечной программы. Такие условия выводятся из исходной (непреобразованной) программы компилятором для последующей проверки правильности конечной программы.

Условия корректности утверждают, что в каждом переходе из $B_{i}$ в $B_{j}$, при условии соблюдения инварианта $\phi_{i}$ и соотношений между переменными, после перехода соблюдается инвариант $\phi_{j}$ и соотношения между переменными сохраняются.

Работа \cite{ZPFG02} содержит обсуждение, доказательство правильности и примеры применения правила $Validate$, приведённого выше.

Проверка созданных алгоритмом $Validate$ условий производится с помощью специальных программ, которые формально доказывают теоремы. Примером такой программы является используемая в работе \cite{PRSS99}.

\section{Проверка преобразований, изменяющих структуру программы}

Преобразование, изменяющее структуру программы --- любое преобразование, которое изменяет порядок исполнения программы без добавления или удаления операторов \cite{Bacon,AK02}. Оно сохраняет семантику программы в случае сохранения зависимостей между операторами.

\subsection{Обзор преобразований, изменяющих структуру программы}

Обозначим через $I = \{i_1, \ldots, i_N \}$ множество значений переменной $i = (i_1, \ldots, i_k)$. Рассмотрим цикл

\begin{center}
	\begin{quote}
		\begin{verbatim}
            for i1 = 1 to k1 do
                ...
                for im = 1 to km do
                    B1(i1 ,...,im)
                    ...
                    Bl(i1 ,...,im)
                end
                ...
            end
		\end{verbatim}
	\end{quote}
\end{center}

Исполнение такого цикла может быть описано в виде

\begin{equation}\label{eq:loop-execution}
	\underbrace{B1(i_{1}), B2(i_{1}), \ldots, B\ell(i_{1}),}_{B(i_{1})} 
	\hspace{4mm} \ldots \hspace{4mm} 
	\underbrace{B1(i_{N}), B2(i_{N}), \ldots, B\ell(i_{N})}_{B(i_{N})}.
\end{equation}

Здесь $Bj(i_K)$ --- операторы, исполняемые в теле цикла на итерации $i_K$, а $B(i_K)$ --- последовательность операторов, исполняемая на этой итерации. Преобразование, изменяющее структуру, вызывает изменение порядка выполнения цикла таким образом, что оно является перестановкой выражения (\ref{eq:loop-execution}). 

При этом возможно изменение порядка как между операторами в пределах одной итерации, так и между разными итерациями. Таким образом получаем два~класса преобразований.

Преобразования первого класса меняют расположение итераций тела цикла $B(i_K)$, но не меняют порядок операторов внутри $B(i_K)$. Примеры таких преобразований --- обращение цикла, обмен циклов, разворачивание цикла.

Преобразования второго класса изменяют порядок исполнения операторов $Bj(i_K)$ внутри итерации тела цикла $B(i_K)$, и оставляют в том же порядке сами итерации $B(i_K)$. К таким преобразованиям относится слияние циклов.

Большинство работ рассматривает первый класс преобразований. Каждому преобразованию этого класса может быть поставлена в соответствие характеристическая перестановка, обозначающая соотношение между индексами массивов в исходном коде и целевом коде.

Более подробное рассмотрение оптимизаций, преобразующих циклы, можно найти в источниках \cite{ZuckPFGH02,Bacon}.

\subsection{Правила перестановок}

Доказательство с помощью алгоритма $Validate$, схема которого была приведена в разделе \ref{ss:validate}, может быть применено только к программам, у которых путь управления всегда один и тот же. Таким образом, он не может применяться для проверки многих преобразований циклов. В этом разделе мы рассмотрим правила перестановок для проверки преобразований, изменяющих порядок операторов.

Обычно такие преобразования применяются локально, а остальная часть программы оптимизируется только преобразованиями, сохраняющими структуру. Поэтому формально доказать корректность можно с помощью сочетания классического подхода к проверке правильности программ с помощью алгоритма $Validate$ и правил перестановок.

Введём необходимую терминологию.

\label{def-use}
\emph{Определением} переменной называется факт присваивания ей некоторого значения. \emph{Использованием} переменной называется факт извлечения её текущего значения для использования в каком-либо выражении. Пример: в выражении $y = x + 1$ происходит определение переменной $y$ и использование переменной $x$.

\emph{Зависимостью по данным} называется требование соблюдения порядка между определениями и использованиями переменных.
\emph{Прямой зависимостью} называется такая зависимость по данным, при которой значений переменной $b$ не может быть определено до тех пор, пока не определено значение переменной $a$. Пример на псевдо-языке приведён ниже.

\begin{figure}[!Hb]
    \label{fig:direct-dependency}
    \begin{verbatim}
        a = 0;     (1)
        b = a + 1; (2)
        c = b + 2; (3)
    \end{verbatim}
    Листинг \ref{fig:direct-dependency}
\end{figure}

В программе, приведённой в листинге \ref{fig:direct-dependency}, существуют 2 прямые зависимости: значение переменной $b$ в строке 2 зависит от значения переменной $a$, а значение переменной $c$ в строке 3 зависит от значения переменной $b$.

Далее рассматриваются только прямые зависимости по данным. В таком случае преобразование является корректным, если порядок использования переменной относительно определения этой переменной остаётся неизменным.

Более подробное описание правил перестановки с примерами можно найти в работе \cite{ZuckPFGH02}.

\subsection{Автоматическая проверка}

Проверка преобразований на основе правил перестановки включает несколько этапов, перечисленных ниже.

\begin{itemize}
	\item Доказательство правильности правил перестановки. Оно осуществляется с помощью программ, формально доказывающих теоремы. Одной из таких программ является PVS \cite{SOR93}.
	\item Доказательство правильности преобразования и корректности перестановки. Задача осуществления перестановки решается компилятором и подразумевается, что это осуществляется правильно, без доказательства. Это обусловлено тем, что в большинстве случаев корректность преобразований непосредственно наблюдаема человеком, а также может быть установлена автоматически. Поскольку программа выводит результаты работы в файл (или на экран), правильность её работы можно оценить по правильности полученных результатов.
	\item Нахождение пересечения множеств определений и использований переменных. Таким образом происходит определение того, какие переменные используются в определениях самих себя. Это необходимо для обнаружения рекуррентных зависимостей между итерациями цикла, потому что в таком случае преобразования, изменяющие порядок следования итераций, не являются корректными. Эта задача также решается компилятором и подразумевается корректность её решения.
\end{itemize}

В источнике \cite{ZPL00} можно также найти правила перестановки, работающие с более широким кругом преобразований.

Далее приводится обзор более продвинутых методов автоматической проверки программ.

\section{Динамическая проверка спекулятивных оптимизаций}

Этот раздел представляет собой обзор проверки \emph{спекулятивных оптимизаций циклов во время исполнения}. В случае, когда ни инструмент проверки, ни компилятор не могут гарантировать корректность преобразования, для проверки верности цикловых оптимизаций используются условия времени выполнения. Эта методика особенно полезна при совпадении областей памяти, адресуемых разными переменными (так называемый эффект \emph{memory aliasing}), что не позволяет применить статический анализ заивисимостей.

В отличие от проверки компилятора, динамическая проверка должна обеспечивать не только контроль правильности работы, но и восстановление безошибочного варианта программы в реальном времени в случае некорректности произведённой спекулятивной оптимизации без произведения неверных результатов. В некоторых случаях возможен выбор оптимизированной версии, которая работает безошибочно; в других же необходим откат к полностью неоптимизированной версии.

Динамическая проверка осуществляется с использованием правил перестановки из предыдущего раздела.

\subsection{Безопасность динамической проверки}

Ниже перечислены свойства, которые должны быть сохранены при введении проверки времени выполнения.

\begin{itemize}
	\item Проверка должна обнаруживать, точно или консервативно, нарушение зависимостей по данным в оптимизированном цикле. Поясним точность и консервативность проверки. Определим \emph{ложное  срабатывание} проверки как случай, когда в действительности нарушения зависимостей не произошло, но проверяющий алгоритм выдал сообщение о том, что проверка провалена. Аналогично, \emph{ложное  несрабатывание} --- отсутствие сообщения от алгоритма проверки при действительном нарушении зависимостей. Тогда, точное обнаружение --- обнаружение, при котором  ложные срабатывания и верные несрабатывания отсутствуют. Консервативное обнаружение может допускать ложные  срабатывания, но не допускает ложных несрабатываний.
	\item Как только динамическая проверка обнаруживает возможное нарушение зависимостей, должен быть произведён переход на путь управления, который приведёт к произведению верного результата.
	\item Динамическая проверка должна быть способна определить нарушение зависимости до того, как оно произошло.
\end{itemize}

Последнее свойство, возможно, является слишком строгим. Оно может быть заменено требованием в случае обнаружения нарушения возможности исполнить код, который исправит неверные результаты уже после того, как произошло нарушение.

\subsection{Проблемы эффективности динамической проверки}

Для того, чтобы иметь практический смысл, динамическая проверка должна иметь характеристики, приведённые далее.

\begin{itemize}
	\item Проверка времени выполнения должна происходить как можно более редко.
	\item Проверка должна быть как можно более дешёвой в терминах вычислительной сложности и занимаемой памяти.
	\item Стоимость исполнения цикла в случае обнаружения потенциального нарушения должна быть не больше, чем стоимость исполнения оригинального (неоптимизированного) цикла.
\end{itemize}

В работе \cite{ZuckPFGH02} разработан набор методик динамической оптимизации, которые в разной степени удовлетворяют этим требованиям.

\subsection{Обнаружение нарушения зависимости}

Обнаружение нарушения до того, как оно произошло, представляет определённые сложности. Без учёта этого ограничения можно использовать довольно эффективный алгоритм для проверки корректности оптимизированного цикла \cite{ZuckPFGH02}. Этот алгоритм прост в реализации, но имеет две отрицательные черты:

\begin{enumerate}
	\item он требует столько же дополнительной памяти, каков размер итерируемого массива;
	\item он не обнаруживает нарушения до того, как оно произошло.
\end{enumerate}

Вторая черта делает алгоритм бесполезным в реальной динамической проверке, поскольку если на какой"~то итерации производится неверное значение, затем оно используется на следующей, делая все результаты работы программы неверными.

\subsection{Ограничение множества определений}

В большинстве случаев одно из множеств --- определений или использований (эти понятия определены в разделе \ref{def-use}) --- может быть проанализировано статически. И хотя полностью статический анализ в таком случае невозможен, данная ситуация значительно проще той, в которой оба множества не поддаются анализу. В таком случае возможен анализ с помощью компилятора, как описано в работе \cite{ZuckPFGH02}.

Дальнейшие примеры преобразований и их проверки можно также найти в источнике \cite{ZuckPFGH02}.

\subsection{Рассмотрение архитектурных особенностей}

Создание проверок времени выполнения становится более важным ввиду распространения новых классов процессоров, которые обладают чертами, приведёнными ниже.

\begin{itemize}
	\item Предоставляют возможности эксплуатировать параллелизм на уровне инструкций.
	\item Имеют аппаратные особенности, делающие незначительными расходы на проверку и исправление результатов в случае нарушения зависимостей.
\end{itemize}

Большинство современных процессоров имеют множество функциональных единиц для эксплуатации параллелизма на уровне инструкций. Это достигается с помощью динамического планирования исполнения нескольких инструкций одновременно \cite{dynamic-scheduling} (на суперскалярных машинах) или с помощью сгенерированных компилятором параллельных инструкций \cite{vliw} (в случае VLIW"~процессоров). Сложностью использования VLIW"~машин оказалась трудность нахождения достаточного параллелизма на уровне инструкций.

Поясним предыдущее утверждение. Под \emph{параллелизмом на уровне инструкций} понимается возможность параллельного исполнения инструкций, расположенных последовательно в программе, в случае отсутствия зависимостей между данными, используемыми этими инструкциями. Таким образом можно достичь более полного использования функциональных единиц машины. Ниже приведён пример на псевдо-языке.

\begin{figure}[H]
    \begin{verbatim}
        add a, b, c;
        ld d, e; 
        mul f, g, h;
    \end{verbatim}
    \label{fig:ilp}
    \caption{Код примера}
\end{figure}

Здесь по очереди выполняется сложение $a = b + c$, затем загрузка $d = memory(e)$, после чего умножение $f = g \cdot h$. Как мы видим, зависимости по данным отсутствуют --- все инструкции пользуются различными исходными данными и записывают результат в разные переменные. В суперскалярных машинах аппаратно реализованный планировщик рассматривает последовательность инструкций и в случае отсутствия зависимостей планирует параллельное исполнение.

В архитектуре VLIW используется другой подход: параллелизм определяется на этапе компиляции и создаваемая в нашем примере программа будет иметь другой вид.

\begin{figure}[H]
    \begin{verbatim}
    add a, b, c; ld d, e; mul f, g, h;
    \end{verbatim}
    \label{fig:vliw}
    \caption{Код примера}
\end{figure}

Здесь компилятору удалось определить отсутствие зависимостей по данным статически, во время генерации машинного кода. Он создал длинную инструкцию, включающую в себя указания модулю сложения, модулю загрузки из памяти и модулю умножения. То есть, одна инструкция кодирует 3 действия --- для каждой функциональной единицы процессора, задействованной в данной точке исполнения программы.

В общем случае создать параллельный код, подобный приведённому выше, сложно, поскольку разные имена могут указывать на одну и ту же область памяти (происходит так называемый memory aliasing). Если переменная $a$ расположена по тому же адресу, что и $e$, то $e$ является псевдонимом (англ. alias) для $a$. В нашем примере, имена $a$ и $e$ могут указывать на область памяти с адресом 0x100. Тогда параллельное исполнение инструкций сложения и загрузки является нарушением семантики программы, поскольку адрес для загрузки (переменная $e$) вычисляется инструкцией сложения. В таком случае, обнаружение зависимости по данным серьёзно осложняется.

При этом, возникновение эффекта memory aliasing очень распространено. Следующий код на языке C создаёт два псевдонима (указателя) для области памяти, занимаемой переменной $a$ --- $x$ и $y$.

\begin{figure}[H]
    \begin{verbatim}
    int a = 20;
    int *x = &a;
    int *y = &a;
    \end{verbatim}
    \label{fig:aliasing}
    \caption{Код примера}
\end{figure}

Теперь манипуляции и с $x$, и с $y$ осуществляют действия с одной и той же областью памяти, занимаемой $a$.

\begin{figure}[H]
    \begin{verbatim}
    *x = 8;
    *y = 10;
    \end{verbatim}
    \label{fig:aliasing-2}
    \caption{Код примера}
\end{figure}

Пример выше показывает два присваивания. После исполнения обоих операторов значение области памяти, на которое указывает $x$, будет равно 10. Это происходит потому, что и $x$, и $y$ указывают на самом деле на одну и ту же область памяти. Такие эффекты и осложняют анализ потока данных.

Более того, это только один из примером сложностей, с которыми сталкивается компилятор для архитектуры VLIW. Вообще, во многих случаях компилятор оказывается неспособен создать эффективный машинный код для VLIW-машины, что серьёзно ограничило применение таких процессоров.

Вернёмся к динамической проверке правильности исполнения.

Динамическая проверка может использовать незадействованные в вычислениях модули процессора. Например, для проверки верности инвариантов вида $x == 1$ (ситуации, в которых возникают такие инварианты, описаны в разделе \ref{ss:invariants}) нужно исполнять инструкции сравнения. Во многих современных процессорах для выполнения сравнений используется отдельная функциональная единица. Соответственно, функциональная единица сравнения может выполнять проверку инвариантов параллельно с исполнением основной программы. В случае, если точки проверки инвариантов расположены так, что модуль проверки используется только тогда, когда он не задействован в основной программе, динамическая проверка правильности исполнения не замедлит выполнение проверяемой программы.

Некоторые аппаратные характеристики архитектур, таких, как Intel IA"~64, позволяют улучшить поддержу динамической проверки корректности. В частности, это аппаратно реализованные инструкции динамической проверки на адресацию одной и той же области памяти многими указателями. Иными словами, эта архитектура позволяет аппаратно определять, произошло ли создание псевдонима для области памяти, что может упрощать динамическую проверку. Упрощение достигается за счёт того, что при динамической проверке инвариант может накладывать условие на переменную, которая является псевдонимом для другой переменной. Возвращаясь к примеру из листинга \ref{fig:ilp}, если $e$ --- псевдоним для $a$ и в динамической проверке встречается инвариант $e == 5$, то алгоритм проверки должен сообщать об ошибке, когда значения $a$ и $e$ не совпадают. Они должны совпадать, т.к. оба имени указывают на одну и ту же область памяти. Несовпадение значений указывает на ошибочное преобразование программы. Более подробно об этом можно прочитать в статье \cite{GHCP02}.

Возможность устанавливать предикаты на пути исполнения этой архитектуры также помогает избежать исполнения некорректного кода до того, как оно произошло. Предикаты --- это дополнительные условия исполнения инструкций, проверяемые аппаратно. Рассмотрим пример на псевдокоде.

\begin{figure}[H]
    \begin{verbatim}
    i = 0;
    a[i] = i; PREDICATE: i < 10;
    i = i + 1;
    \end{verbatim}
    \label{fig:predicate}
    \caption{Код примера}
\end{figure}

Здесь первая инструкция присваивает значение $0$ переменной $i$. На следующую инструкцию предикатом наложено условие $i < 10$. Это означает, что процессор проверяет значение $i$ и не исполняет эту инструкцию, если условие в предикате не выполнено. Последняя инструкция производит безусловное увеличение $i$ на единицу.

\section*{Заключение}
\addcontentsline{toc}{section}{Заключение}%

Мы рассмотрели подходы к формальной проверке правильности программ, а именно статическую проверку и динамическую проверку, и сделали обзор механизмов реализации проверки.

Мы рассмотрели способы проверки корректности оптимизаций циклов и предложили специальную методику для преобразований, изменяющих порядок исполнения операторов, состоящую в добавлении проверяющего и корректирующего кода в компилируемую программу. Этот дополнительный код обнаруживает нарушения зависимостей и предотвращает порчу результатов работы программы.