\section{Проверка оптимизаций, сохраняющих структуру программы}

Опишем стратегию инструмента проверки \emph{voc} \cite{ZPFG02} и теорию проверки оптимизаций.

Компилятор получает на вход \emph{исходную программу}, написанную на высокоуровневом языке, преобразует её в \emph{промежуточное представление} и затем применяет набор оптимизаций к программе --- начиная с классических глобальных оптимизаций, независящих от архитектуры, и заканчивая архитектурно"~зависимыми, такими как распределение регистров и планирование инструкций. Обычно эти оптимизации производятся в несколько проходов (до 15 в некоторых компиляторах), и каждый проход применяет оптимизацию определённого типа.

Для того, чтобы доказать, что целевая программа является корректной трансляцией исходной программы, рассмотрим сначала некоторые необходимые термины. Определим, что целевая программа \emph{T} является корректной трансляцией исходной программы \emph{S}, если исполнение программы \emph{T} с некоторыми значениями переменных эквивалентно исполнению программы \emph{S} с некоторыми другими значениями переменных.

Промежуточное представление --- это трёхадресный код. Он описан \emph{графом потока}. Каждый узел в графе потока представляет собой \emph{линейный участок}, то есть последовательность операторов, не содержащую ветвлений. Границы линейного участка определяются потоком управления.

\subsection{Системы переходов}

Для доказательства правильности преобразования исходного кода, выделим сначала семантически эквивалентные последовательности операций в коде до преобразования и после. Это осуществляется с помощью \emph{системы переходов} (англ. transition system, $TS$; вариант понятия, аналогичного упомянутому в работе \cite{PSS98b}). Система переходов $S = \left\langle  V, O, \Theta, \rho \right\rangle $ --- конечный автомат, где $V$ --- множество переменных состояния, $O \subseteq V$ --- множество наблюдаемых переменных состояния, $\Theta$ --- начальное состояние системы, $\rho$ --- правила перехода, сопоставляющие исходное и результирующее состояние.

Значение, получаемое переменной $x$ в состоянии $s$, обозначается $s[x]$. Правило перехода $y' = y + 1$ означает, что в новом состоянии значение переменной $y$ на 1 больше, чем в старом.

Наблюдаемыми считаются все переменные, значение которых выводится на внешние устройства в результате работы программы.

Система переходов называется детерминированной, если начальное состояние однозначно определяет дальнейший ход вычислений. Мы рассматриваем проверку только таких программ.

Определим $P_{S} = \left\langle  V_{s}, O_{s}, \Theta_{s}, \rho_{s} \right\rangle $ и $P_{T} = \left\langle  V_{t}, O_{t}, \Theta_{t}, \rho_{t} \right\rangle $ как исходную и целевую систему переходов, соответственно. 
Они называются сравнимыми, если существует соотношение один"~к"~одному между наблюдаемыми переменными $O_{S}$ системы $P_{S}$ и наблюдаемыми переменными $O_{T}$ системы $P_{T}$. Обозначим $ X \subset O_{s} $ и $ x \subset O_{t} $ соответствующие наблюдаемые переменные. 
Исходное состояние \emph{совместимо} с целевым, если они оба включают одинаковые множества наблюдаемых переменных. Обозначим $\sigma_{S}: s_{0}, s_{1}, \ldots, s_{m}$ вычисление в исходной системе переходов $P_{S}$ и $\sigma_{T}: t_{0}, t_{1}, \ldots, t_{n}$ вычисление в целевой системе переходов $P_{T}$. 

{
    Рассмотрим условия, при которых трансляция программы считается правильной.
    \begin{enumerate}
        \item {Пусть $P_{S}$ и $P_{T}$ сравнимы. \label{st:comparable}}
        \item {Положим также, что начальные состояния $s_{0}$ и $t_{0}$ вычислений $\sigma_{S}$ и $\sigma_{T}$ соответственно совместимы. \label{st:compatible}}
        \item {Будем считать, что если вычисление $\sigma_{S}$ завершается, то вычисление $\sigma_{T}$ также должно завершаться и их конечные состояния $s_{m}$ и $t_{n}$ соответственно должны быть совместимы. \label{st:terminating}}
    \end{enumerate}
    
    В случае выполнения условий \ref{st:comparable}, \ref{st:compatible}, \ref{st:terminating} система $P_{T}$ является правильной трансляцией системы $P_{T}$.
}

\subsection{Проверка оптимизаций}
\label{ss:validate}

Положим $P_{S}$ и $P_{T}$ сравнимыми системами переходов. Для формального доказательства корректности преобразования $P_{s}$ в $P_{t}$ введём правило доказательства $Validate$, приведённое ниже. Оно включает в себя установление соответствия потока управления между исходным и целевым кодом и установление соответствия данных между исходным и целевым кодом.

Далее для краткости будем пользоваться термином «участок» как синонимом термина «линейный участок».

Обе системы переходов имеют множество разрезов $CP$. Множество разрезов $CP$ представляет собой множество участков, включающих начальный и конечный участок и хотя бы один участок из каждого цикла в графе потока управления программы. Простой путь --- путь, соединяющий два разреза, и не содержащий больше никаких разрезов в качестве промежуточных узлов.

Далее приводится схема алгоритма $Validate$. За ней следуют дополнительные пояснения.

\begin{enumerate}
    \item Установить соответствие потока управления $CP_{T} \rightarrow CP_{S}$ между начальным и конечным участком в целевом коде и соответствующими участками в исходном коде.
    \item Для каждого участка $B_{i}$ в $CP_{T}$ сформировать инвариант $\phi_{i}$ (см. пояснения после текста схемы алгоритма). \label{st:invariant}
    \item Назначить выражения, содержащие переменные состояния из целевого представления программы, в соответствие переменным. Эти выражения зависят от значений предикатов. Значения предикатов вычисляются из переменных в целевом представлении программы. Таким образом устанавливается соответствие между данными в исходном представлении и целевом представлении программы.
    \item Для каждой пары линейных участков $B_{i}$ и $B_{j}$, для которых существует простой путь в графе потока управления, создать множество простых путей, соединяющих эти линейные участки в графе потока данных, и условие корректности.
    \item Установить верность всех созданных условий корректности трансляции.
\end{enumerate}

Для работы алгоритма программа дополнена знаниями о выполнении условий, наложенных на определённые переменные. Эти знания называются \emph{аннотациями} и имеют вид инвариантов $\phi_{i}$ в пункте \ref{st:invariant} правила. Инвариант --- это условие, которое должно соблюдаться независимо от преобразований, выполняемых над программой.

Имеют место разные аннотации для разных линейных участков. Аннотации программы, закреплённые инвариантом $\phi_{i}$, верны при исполнении блока $B_{i}$. Инварианты могут быть получены путём анализа потока данных, который выполняется компилятором. Они представляют собой условия вида $x == 1$, которые должны быть верны в определённых точках исполнения как исходной, так и конечной программы. Такие условия выводятся из исходной (непреобразованной) программы компилятором для последующей проверки правильности конечной программы.

Условия корректности утверждают, что в каждом переходе из $B_{i}$ в $B_{j}$, при условии соблюдения инварианта $\phi_{i}$ и соотношений между переменными, после перехода соблюдается инвариант $\phi_{j}$ и соотношения между переменными сохраняются.

Работа \cite{ZPFG02} содержит обсуждение, доказательство правильности и примеры применения правила $Validate$, приведённого выше.

Проверка созданных алгоритмом $Validate$ условий производится с помощью специальных программ, которые формально доказывают теоремы. Примером такой программы является используемая в работе \cite{PRSS99}.

\section{Проверка преобразований, изменяющих структуру программы}

Преобразование, изменяющее структуру программы --- любое преобразование, которое изменяет порядок исполнения программы без добавления или удаления операторов \cite{Bacon,AK02}. Оно сохраняет семантику программы в случае сохранения зависимостей между операторами.

\subsection{Обзор преобразований, изменяющих структуру программы}

Обозначим через $I = \{i_1, \ldots, i_N \}$ множество значений переменной $i = (i_1, \ldots, i_k)$. Рассмотрим цикл

\begin{center}
	\begin{quote}
		\begin{verbatim}
            for i1 = 1 to k1 do
                ...
                for im = 1 to km do
                    B1(i1 ,...,im)
                    ...
                    Bl(i1 ,...,im)
                end
                ...
            end
		\end{verbatim}
	\end{quote}
\end{center}

Исполнение такого цикла может быть описано в виде

\begin{equation}\label{eq:loop-execution}
	\underbrace{B1(i_{1}), B2(i_{1}), \ldots, B\ell(i_{1}),}_{B(i_{1})} 
	\hspace{4mm} \ldots \hspace{4mm} 
	\underbrace{B1(i_{N}), B2(i_{N}), \ldots, B\ell(i_{N})}_{B(i_{N})}.
\end{equation}

Здесь $Bj(i_K)$ --- операторы, исполняемые в теле цикла на итерации $i_K$, а $B(i_K)$ --- последовательность операторов, исполняемая на этой итерации. Преобразование, изменяющее структуру, вызывает изменение порядка выполнения цикла таким образом, что оно является перестановкой выражения (\ref{eq:loop-execution}). 

При этом возможно изменение порядка как между операторами в пределах одной итерации, так и между разными итерациями. Таким образом получаем два~класса преобразований.

Преобразования первого класса меняют расположение итераций тела цикла $B(i_K)$, но не меняют порядок операторов внутри $B(i_K)$. Примеры таких преобразований --- обращение цикла, обмен циклов, разворачивание цикла.

Преобразования второго класса изменяют порядок исполнения операторов $Bj(i_K)$ внутри итерации тела цикла $B(i_K)$, и оставляют в том же порядке сами итерации $B(i_K)$. К таким преобразованиям относится слияние циклов.

Большинство работ рассматривает первый класс преобразований. Каждому преобразованию этого класса может быть поставлена в соответствие характеристическая перестановка, обозначающая соотношение между индексами массивов в исходном коде и целевом коде.

Более подробное рассмотрение оптимизаций, преобразующих циклы, можно найти в источниках \cite{ZuckPFGH02,Bacon}.

\subsection{Правила перестановок}

Доказательство с помощью алгоритма $Validate$, схема которого была приведена в разделе \ref{ss:validate}, может быть применено только к программам, у которых путь управления всегда один и тот же. Таким образом, он не может применяться для проверки многих преобразований циклов. В этом разделе мы рассмотрим правила перестановок для проверки преобразований, изменяющих порядок операторов.

Обычно такие преобразования применяются локально, а остальная часть программы оптимизируется только преобразованиями, сохраняющими структуру. Поэтому формально доказать корректность можно с помощью сочетания классического подхода к проверке правильности программ с помощью алгоритма $Validate$ и правил перестановок.

Далее рассматриваются только прямые зависимости потока данных. Преобразование является корректным, если порядок использования переменной относительно определения этой переменной остаётся неизменным.

Более подробное описание правил перестановки с примерами можно найти в работе \cite{ZuckPFGH02}.

\subsection{Автоматическая проверка}

Проверка преобразований на основе правил перестановки включает несколько этапов, перечисленных ниже.

\begin{itemize}
	\item Доказательство правильности правил перестановки. Оно осуществляется с помощью формальных доказателей теорем вроде PVS \cite{SOR93}.
	\item Доказательство правильности преобразования и корректности перестановки. Задача осуществления перестановки решается компилятором и подразумевается, что это осуществляется правильно, без доказательства, поскольку эмпирически наблюдаема корректность преобразований в большинстве случаев.
	\item Решение СЛАУ для определения пересечения множеств определений и использований переменных. Эта задача также решается компилятором и подразумевается корретность осуществления решения.
\end{itemize}

В источнике \cite{ZPL00} можно также найти правила перестановки, работающие с более широким кругом преобразований.

\section{Динамическая проверка спекулятивных оптимизаций}

Этот раздел представляет собой обзор проверки \emph{спекулятивных оптимизаций циклов во время исполнения}. В случае, когда ни инструмент проверки, ни компилятор не могут гарантировать корректность преобразования, для проверки верности цикловых оптимизаций используются условия времени выполнения. Эта методика особенно полезна при совпадении областей памяти, адресуемых разными переменными (т.н. \emph{memory aliasing}), что не позволяет применить статический анализ заивисимостей.

В отличие от проверки компилятора, динамическая проверка должна обеспечивать не только контроль правильности работы, но и восстановление безошибочного варианта программы в реальном времени в случае некорректности произведённой спекулятивной оптимизации без произведения неверных результатов. В некоторых случаях возможен выбор оптимизированной версии, которая работает безошибочно; в других же необходим откат к полностью неоптимизированной версии.

Динамическая проверка осуществляется с использованием правил перестановки из предыдущего раздела.

\subsection{Безопасность динамической проверки}

Ниже перечислены свойства, которые должны быть сохранены при введении проверки времени выполнения.

\begin{itemize}
	\item Проверка должна обнаруживать, точно или консервативно, нарушение зависимостей по данным в оптимизированном цикле.
	\item Как только динамическая проверка обнаруживает возможное нарушение зависимостей, должен быть произведён переход на путь управления, который приведёт к произведению верного результата.
	\item Динамическая проверка должна быть способна определить нарушение зависимости до того, как оно произошло.
\end{itemize}

Последнее свойство, возможно, является слишком строгим. Оно может быть заменено требованием в случае обнаружения нарушения пост"~фактум возможности исполнить код, который исправит неверные результаты.

\subsection{Проблемы эффективности динамической проверки}

Для того, чтобы иметь практический смысл, динамическая проверка должна иметь качественные характеристики, приведённые далее.

\begin{itemize}
	\item Проверка времени выполнения должна происходить как можно более редко.
	\item Проверка должна быть как можно более дешёвой в терминах вычислительной сложности и занимаемой памяти.
	\item Стоимость исполнения цикла в случае обнаружения потенциального нарушения должна быть не больше, чем стоимость исполнения оригинального (неоптимизированного) цикла.
\end{itemize}

В работе \cite{ZuckPFGH02} разработан набор методик динамической оптимизации, которые в разной степени удовлетворяют этим требованиям.

\subsection{Обнаружение нарушения зависимости}

Обнаружение нарушения до того, как оно произошло, представляет определённые сложности. Без учёта этого ограничения можно использовать довольно эффективный алгоритм для проверки корректности оптимизированного цикла. Его можно найти в источнике \cite{ZuckPFGH02}. Этот алгоритм прост в реализации, но имеет 2 отрицательные черты:

\begin{enumerate}
	\item он требует столько же дополнительной памяти, каков размер итерируемого массива;
	\item он не обнаруживает нарушения до того, как оно произошло.
\end{enumerate}

Вторая черта делает алгоритм бесполезным в реальной динамической проверке, поскольку если на какой"~то итерации производится неверное значение, затем оно используется на следующей, делая все результаты работы программы неверными.

\subsection{Ограничение множества определений}

В большинстве случаев одно из множеств --- определений или использований --- может быть проанализировано статически. И хотя полностью статический анализ в таком случае невозможен, данная ситуация значительно проще той, в которой оба множества не поддаются анализу. В таком случае возможен анализ с помощью компилятора, как описано в работе \cite{ZuckPFGH02}.

Дальнейшие примеры преобразований и их проверки можно также найти в источнике \cite{ZuckPFGH02}.

\subsection{Архитектурные соображения}

Создание проверок времени выполнения становится более важным ввиду распространения новых классов процессоров, которые обладают чертами, приведёнными ниже.

\begin{itemize}
	\item Предоставляют возможности эксплуатировать параллелизм на уровне инструкций;
	\item Имею аппаратные особенности, делающие незначительными расходы на проверку и исправление результатов в случае нарушения зависимостей.
\end{itemize}

Большинство современных процессоров имеют множество функциональных единиц для эксплуатации параллелизма на уровне инструкций с помощью динамического планирования исполнения нескольких инструкций одновременно на суперскалярных машинах или с помощью сгенерированных компилятором параллельных инструкций (в случае VLIW"~процессоров). Сложностью использования VLIW"~машин оказалась трудность нахождения достаточного параллелизма на уровне инструкций с целью использовать все доступные функциональные единицы машины. Таким образом, динамическая проверка может использовать незадействованные в вычислениях модули процессора.

Некоторые аппаратные характеристики архитектур, таких, как Intel IA"~64, позволяют улучшить поддержу динамической проверки корректности. В частности, это аппаратно реализованные инструкции динамической проверки на адресацию одной и той же области памяти многими указателями. Возможность устанавливать предикаты на пути исполнения этой архитектуры также помогает избежать исполнения некорректного кода до того, как оно произошло. Более подробно об этом можно прочитать в статье \cite{GHCP02}.

\section*{Заключение}
\addcontentsline{toc}{section}{Заключение}%

Мы рассмотрели подходы к формальной проверке правильности программ, а именно статическую проверку и динамическую проверк; их сильные и слабые стороны и сделали обзор механизмов реализации проверки.

Мы рассмотрели способы проверки корректности цикловых оптимизаций и предложили специальную методику для преобразований, изменяющих порядок исполнения операторов, состояющую в добавлении проверяющего и корректирующего кода в компилируемую программу. Этот инструментирующий код обнаруживает нарушения зависимостей и предотвращает порчу результатов работы программы.