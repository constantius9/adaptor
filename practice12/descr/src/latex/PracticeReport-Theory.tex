\part{Теоретическая часть}

\section{Регрессионный анализ}

К регрессионному анализу относятся задачи выявления искажённой случайным «шумом» функциональной зависимости интересующего исследователя показателя $Y$ от измеряемых переменных $X_1,...,X_m$. Данными служит таблица экспериментально полученных «зашумлённых» значений $Y$ на разных наборах $x_1,...,x_m$. \cite{lag}

\subsection{Цели регрессионного анализа}

\begin{itemize}
\item Определение степени детерминированности вариации критериальной (зависимой) переменной предикторами (независимыми переменными).
\item Предсказание значения зависимой переменной с помощью независимой(-ых).
\item Определение вклада отдельных независимых переменных в вариацию зависимой.
\end{itemize}

Регрессионный анализ нельзя использовать для определения наличия связи между переменными, поскольку наличие такой связи и есть предпосылка для применения анализа.

\subsection{Математическое определение регрессии}

Строго регрессионную зависимость можно определить следующим образом. Пусть $Y, X_1, X_2,...,X_p$  — случайные величины с заданным совместным распределением вероятностей. Если для каждого набора значений $X_1=x1,X_2=x_2,...,X_p=x_p$ определено условное математическое ожидание
\begin{equation}
y(x_1,x_2,...,x_p) = E(Y|X_1 = x_1, X_2 = x_2, ..., X_p = x_p)
\end{equation}
(уравнение регрессии в общем виде), то функция $y(x_1,x_2,...,x_p)$ называется регрессией величины Y по величинам $X_1,X_2,...,X_p$, а её график — линией регрессии $Y$ по $X_1,X_2,...,X_p$, или уравнением регрессии \cite{applregr}.
Зависимость $Y$ от $X_1,X_2,...,X_p$ проявляется в изменении средних значений $Y$ при изменении $X_1,X_2,...,X_p$. Хотя при каждом фиксированном наборе значений $X_1,X_2,...,X_p$ величина $Y$ остаётся случайной величиной с определённым рассеянием.
Для выяснения вопроса, насколько точно регрессионный анализ оценивает изменение $Y$ при изменении $X_1,X_2,...,X_p$, используется средняя величина дисперсии $Y$ при разных наборах значений $X_1,X_2,...,X_p$ (фактически речь идет о мере рассеяния зависимой переменной вокруг линии регрессии).

\subsection{Метод наименьших квадратов (расчёт коэффициентов)}

На практике линия регрессии чаще всего ищется в виде линейной функции $Y = b_0 + b_1 X_1 + b_2 X_2 + ... + b_N X_N$ (линейная регрессия), наилучшим образом приближающей искомую кривую \cite{regrmeth}. Делается это с помощью метода наименьших квадратов, когда минимизируется сумма квадратов отклонений реально наблюдаемых $Y$ от их оценок \textit{\^Y} (имеются в виду оценки с помощью прямой линии, претендующей на то, чтобы представлять искомую регрессионную зависимость):
\begin{equation}
\sum\limits_{k=1}^{M}{(Y_k - \hat{Y}_k)^2} \rightarrow min
\end{equation}

(M — объём выборки). Этот подход основан на том известном факте, что фигурирующая в приведённом выражении сумма принимает минимальное значение именно для того случая, когда $Y = y(x_1,x_2,...,x_N)$.
Для решения задачи регрессионного анализа методом наименьших квадратов вводится понятие функции невязки:
\begin{equation}
\sigma(\bar{b}) = \frac{1}{2} \sum\limits_{k=1}^{M}{(Y_k - \hat{Y}_k)^2}
\end{equation}
Условие минимума функции невязки выражается системой $N+1$  линейных уравнений с $N+1$ неизвестными $b_0...b_n$.
В итоге мы получаем получаем матричное уравнение: $A \times X = B$, которое легко решается методом Гаусса. Полученная матрица будет матрицей, содержащей коэффициенты уравнения линии регрессии:
\begin{equation}
X = \left\{ \begin{array}{c}
b_0\\
b_1\\
...\\
b_N
\end{array} \right\}
\end{equation}
Для получения наилучших оценок необходимо выполнение предпосылок МНК (условий Гаусса-Маркова). В англоязычной литературе такие оценки называются BLUE (Best Linear Unbiased Estimators) --- наилучшие линейные несмещенные оценки.

\subsection{Интерпретация параметров регрессии}

Параметры $b_i$ являются частными коэффициентами корреляции; $(b_i)^2$ интерпретируется как доля дисперсии $Y$, объяснённая $X_i$, при закреплении влияния остальных предикторов, то есть измеряет индивидуальный вклад  в объяснение Y. В случае коррелирующих предикторов возникает проблема неопределённости в оценках, которые становятся зависимыми от порядка включения предикторов в модель. В таких случаях необходимо применение методов анализа корреляционного и пошагового регрессионного анализа \cite{statseval}.
Говоря о нелинейных моделях регрессионного анализа, важно обращать внимание на то, идет ли речь о нелинейности по независимым переменным (с формальной точки зрения легко сводящейся к линейной регрессии), или о нелинейности по оцениваемым параметрам (вызывающей серьёзные вычислительные трудности). При нелинейности первого вида с содержательной точки зрения важно выделять появление в модели членов вида $X_1 X_2$, $X_1 X_2 X_3$, свидетельствующее о наличии взаимодействий между признаками $X_1$, $X_2$ и т. д (см. Мультиколлинеарность).